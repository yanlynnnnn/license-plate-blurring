{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from Stats import Splitter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgFramesLeftTurn</th>\n",
       "      <th>AvgFramesRightTurn</th>\n",
       "      <th>AvgFramesMerge</th>\n",
       "      <th>AvgFramesUTurn</th>\n",
       "      <th>IntersectionMaxAcc</th>\n",
       "      <th>IntersectionAvgAcc</th>\n",
       "      <th>IntersectionVarAcc</th>\n",
       "      <th>IntersectionMaxYaw</th>\n",
       "      <th>IntersectionAvgYaw</th>\n",
       "      <th>IntersectionVarYaw</th>\n",
       "      <th>...</th>\n",
       "      <th>MergeMaxYaw</th>\n",
       "      <th>MergeAvgYaw</th>\n",
       "      <th>MergeVarYaw</th>\n",
       "      <th>UTurnMaxAcc</th>\n",
       "      <th>UTurnAvgAcc</th>\n",
       "      <th>UTurnVarAcc</th>\n",
       "      <th>UTurnMaxYaw</th>\n",
       "      <th>UTurnAvgYaw</th>\n",
       "      <th>UTurnVarYaw</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022657</td>\n",
       "      <td>-0.841095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034730</td>\n",
       "      <td>0.569793</td>\n",
       "      <td>-0.107249</td>\n",
       "      <td>-0.677880</td>\n",
       "      <td>-0.170996</td>\n",
       "      <td>-0.322533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.303128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.814371</td>\n",
       "      <td>-1.018391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.062706</td>\n",
       "      <td>0.447115</td>\n",
       "      <td>-0.703907</td>\n",
       "      <td>1.006835</td>\n",
       "      <td>-1.213577</td>\n",
       "      <td>1.010875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.303128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.256352</td>\n",
       "      <td>-0.663798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.342463</td>\n",
       "      <td>-0.680043</td>\n",
       "      <td>-0.061242</td>\n",
       "      <td>-0.607683</td>\n",
       "      <td>0.184810</td>\n",
       "      <td>-0.363580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.303128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.018391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105148</td>\n",
       "      <td>-0.174470</td>\n",
       "      <td>-0.081112</td>\n",
       "      <td>-0.046111</td>\n",
       "      <td>-0.134974</td>\n",
       "      <td>-0.278161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.303128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.814371</td>\n",
       "      <td>-0.841095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.370438</td>\n",
       "      <td>-0.420949</td>\n",
       "      <td>-0.385951</td>\n",
       "      <td>1.989586</td>\n",
       "      <td>-0.899950</td>\n",
       "      <td>3.905272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.303128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>0.022657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776564</td>\n",
       "      <td>1.722847</td>\n",
       "      <td>0.350910</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>-0.397178</td>\n",
       "      <td>-0.253835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.806791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>0.208663</td>\n",
       "      <td>-0.309205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.276598</td>\n",
       "      <td>0.412880</td>\n",
       "      <td>-0.561455</td>\n",
       "      <td>0.233737</td>\n",
       "      <td>1.428014</td>\n",
       "      <td>0.891668</td>\n",
       "      <td>-0.072600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862328</td>\n",
       "      <td>0.486498</td>\n",
       "      <td>1.014324</td>\n",
       "      <td>-0.270132</td>\n",
       "      <td>-0.263974</td>\n",
       "      <td>0.501930</td>\n",
       "      <td>1.806791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.301666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330281</td>\n",
       "      <td>0.245026</td>\n",
       "      <td>1.014603</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>-0.467290</td>\n",
       "      <td>-0.127200</td>\n",
       "      <td>-0.230330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337165</td>\n",
       "      <td>0.174637</td>\n",
       "      <td>0.217262</td>\n",
       "      <td>0.473558</td>\n",
       "      <td>0.797583</td>\n",
       "      <td>-0.646166</td>\n",
       "      <td>1.806791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-0.256352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.046796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.398414</td>\n",
       "      <td>-1.162508</td>\n",
       "      <td>0.152984</td>\n",
       "      <td>-0.537487</td>\n",
       "      <td>0.358867</td>\n",
       "      <td>-0.362286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210055</td>\n",
       "      <td>-0.126709</td>\n",
       "      <td>-0.321549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.806791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-0.070346</td>\n",
       "      <td>-0.604699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.496807</td>\n",
       "      <td>0.733689</td>\n",
       "      <td>1.279645</td>\n",
       "      <td>-0.607683</td>\n",
       "      <td>-0.284337</td>\n",
       "      <td>-0.313472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.806791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1370 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AvgFramesLeftTurn  AvgFramesRightTurn  AvgFramesMerge  AvgFramesUTurn  \\\n",
       "0              0.022657           -0.841095        0.000000        0.000000   \n",
       "1             -0.814371           -1.018391        0.000000        0.000000   \n",
       "2             -0.256352           -0.663798        0.000000        0.000000   \n",
       "3              0.000000           -1.018391        0.000000        0.000000   \n",
       "4             -0.814371           -0.841095        0.000000        0.000000   \n",
       "...                 ...                 ...             ...             ...   \n",
       "1365           0.022657            0.000000        0.000000        0.000000   \n",
       "1366           0.208663           -0.309205        0.000000       -0.276598   \n",
       "1367           0.301666            0.000000        0.000000        2.330281   \n",
       "1368          -0.256352            0.000000       -0.046796        0.000000   \n",
       "1369          -0.070346           -0.604699        0.000000        0.000000   \n",
       "\n",
       "      IntersectionMaxAcc  IntersectionAvgAcc  IntersectionVarAcc  \\\n",
       "0              -0.034730            0.569793           -0.107249   \n",
       "1              -0.062706            0.447115           -0.703907   \n",
       "2              -0.342463           -0.680043           -0.061242   \n",
       "3               0.105148           -0.174470           -0.081112   \n",
       "4              -0.370438           -0.420949           -0.385951   \n",
       "...                  ...                 ...                 ...   \n",
       "1365            0.776564            1.722847            0.350910   \n",
       "1366            0.412880           -0.561455            0.233737   \n",
       "1367            0.245026            1.014603            0.014600   \n",
       "1368           -0.398414           -1.162508            0.152984   \n",
       "1369            0.496807            0.733689            1.279645   \n",
       "\n",
       "      IntersectionMaxYaw  IntersectionAvgYaw  IntersectionVarYaw  ...  \\\n",
       "0              -0.677880           -0.170996           -0.322533  ...   \n",
       "1               1.006835           -1.213577            1.010875  ...   \n",
       "2              -0.607683            0.184810           -0.363580  ...   \n",
       "3              -0.046111           -0.134974           -0.278161  ...   \n",
       "4               1.989586           -0.899950            3.905272  ...   \n",
       "...                  ...                 ...                 ...  ...   \n",
       "1365            0.024085           -0.397178           -0.253835  ...   \n",
       "1366            1.428014            0.891668           -0.072600  ...   \n",
       "1367           -0.467290           -0.127200           -0.230330  ...   \n",
       "1368           -0.537487            0.358867           -0.362286  ...   \n",
       "1369           -0.607683           -0.284337           -0.313472  ...   \n",
       "\n",
       "      MergeMaxYaw  MergeAvgYaw  MergeVarYaw  UTurnMaxAcc  UTurnAvgAcc  \\\n",
       "0        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "1        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "2        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "3        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "4        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1365     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "1366     0.000000     0.000000     0.000000     0.862328     0.486498   \n",
       "1367     0.000000     0.000000     0.000000     0.337165     0.174637   \n",
       "1368    -0.210055    -0.126709    -0.321549     0.000000     0.000000   \n",
       "1369     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "      UTurnVarAcc  UTurnMaxYaw  UTurnAvgYaw  UTurnVarYaw      Date  \n",
       "0        0.000000     0.000000     0.000000     0.000000 -1.303128  \n",
       "1        0.000000     0.000000     0.000000     0.000000 -1.303128  \n",
       "2        0.000000     0.000000     0.000000     0.000000 -1.303128  \n",
       "3        0.000000     0.000000     0.000000     0.000000 -1.303128  \n",
       "4        0.000000     0.000000     0.000000     0.000000 -1.303128  \n",
       "...           ...          ...          ...          ...       ...  \n",
       "1365     0.000000     0.000000     0.000000     0.000000  1.806791  \n",
       "1366     1.014324    -0.270132    -0.263974     0.501930  1.806791  \n",
       "1367     0.217262     0.473558     0.797583    -0.646166  1.806791  \n",
       "1368     0.000000     0.000000     0.000000     0.000000  1.806791  \n",
       "1369     0.000000     0.000000     0.000000     0.000000  1.806791  \n",
       "\n",
       "[1370 rows x 65 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pickle.load(open('scaled_df.pkl', 'rb'))\n",
    "df = df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:1000]\n",
    "val_df = df[1000:]\n",
    "enrollment_df = val_df[(val_df.index % 10 < 5)]\n",
    "verification_df = val_df[(val_df.index % 10 >= 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_df.to_numpy()\n",
    "train_X = train_data[:, :-1]\n",
    "train_Y = train_data[:, -1]\n",
    "label_encoder = LabelEncoder()\n",
    "train_integer_encoded = label_encoder.fit_transform(train_Y)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "train_integer_encoded = train_integer_encoded.reshape(len(train_integer_encoded), 1)\n",
    "train_Y_encoded = onehot_encoder.fit_transform(train_integer_encoded)\n",
    "train_X = np.reshape(train_X, (1000, 8, 8))\n",
    "train_Y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_23 (Conv1D)           (None, 6, 256)            6400      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 3, 128)            98432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 134,244\n",
      "Trainable params: 134,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "filters = 256\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1, input_shape= (8, 8)))\n",
    "cnn_model.add(MaxPooling1D())\n",
    "cnn_model.add(Conv1D(filters/2, kernel_size, padding='same', activation='relu', strides=1, input_shape= (6, filters)))\n",
    "cnn_model.add(MaxPooling1D())\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(100, activation='softmax'))\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "50/50 [==============================] - 1s 2ms/step - loss: 4.5995 - accuracy: 0.0080\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 4.4373 - accuracy: 0.0290\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 4.2100 - accuracy: 0.0560\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 3.8961 - accuracy: 0.1070\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 3.6312 - accuracy: 0.1260\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 3.3422 - accuracy: 0.1930\n",
      "Epoch 7/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 3.0751 - accuracy: 0.2250\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.8208 - accuracy: 0.2850\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.5902 - accuracy: 0.3380\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.3897 - accuracy: 0.3820\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.2028 - accuracy: 0.4310\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.0465 - accuracy: 0.4750\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.8832 - accuracy: 0.5300\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.7371 - accuracy: 0.5580\n",
      "Epoch 15/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.6125 - accuracy: 0.5960\n",
      "Epoch 16/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.5159 - accuracy: 0.6110\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.3977 - accuracy: 0.6580\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.2981 - accuracy: 0.6730\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.2159 - accuracy: 0.7100\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.1674 - accuracy: 0.7140\n",
      "Epoch 21/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.1253 - accuracy: 0.7160\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.0761 - accuracy: 0.7340\n",
      "Epoch 23/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.0293 - accuracy: 0.7580\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9915 - accuracy: 0.7530\n",
      "Epoch 25/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.9456 - accuracy: 0.7670\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9049 - accuracy: 0.7810\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9244 - accuracy: 0.7630\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9944 - accuracy: 0.7390\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9174 - accuracy: 0.7710\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8402 - accuracy: 0.7870\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8032 - accuracy: 0.7950\n",
      "Epoch 32/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7645 - accuracy: 0.8020\n",
      "Epoch 33/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7428 - accuracy: 0.8080\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.7454 - accuracy: 0.8080\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7543 - accuracy: 0.8070\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7340 - accuracy: 0.8100\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7129 - accuracy: 0.8190\n",
      "Epoch 38/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.8230\n",
      "Epoch 39/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.8200\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6803 - accuracy: 0.8230\n",
      "Epoch 41/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.8230\n",
      "Epoch 42/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6683 - accuracy: 0.8250\n",
      "Epoch 43/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7186 - accuracy: 0.8030\n",
      "Epoch 44/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8988 - accuracy: 0.7650\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8665 - accuracy: 0.7770\n",
      "Epoch 46/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8633 - accuracy: 0.7780\n",
      "Epoch 47/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7754 - accuracy: 0.8000\n",
      "Epoch 48/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7590 - accuracy: 0.8010\n",
      "Epoch 49/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7130 - accuracy: 0.8190\n",
      "Epoch 50/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6617 - accuracy: 0.8210\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.8310\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.8360\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6250 - accuracy: 0.8310\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6289 - accuracy: 0.8350\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6387 - accuracy: 0.8360\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6365 - accuracy: 0.8300\n",
      "Epoch 57/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6231 - accuracy: 0.8420\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6165 - accuracy: 0.8340\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.8340\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6030 - accuracy: 0.8380\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5968 - accuracy: 0.8410\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.8370\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5955 - accuracy: 0.8420\n",
      "Epoch 64/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.8340\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.8400\n",
      "Epoch 66/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5870 - accuracy: 0.8400\n",
      "Epoch 67/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.8350\n",
      "Epoch 68/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5808 - accuracy: 0.8440\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5855 - accuracy: 0.8430\n",
      "Epoch 70/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6130 - accuracy: 0.8400\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5916 - accuracy: 0.8380\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5870 - accuracy: 0.8410\n",
      "Epoch 73/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5812 - accuracy: 0.8420\n",
      "Epoch 74/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6082 - accuracy: 0.8390\n",
      "Epoch 75/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6897 - accuracy: 0.8240\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.4482 - accuracy: 0.6660\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 1.0343 - accuracy: 0.7320\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.8170\n",
      "Epoch 79/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6184 - accuracy: 0.8400\n",
      "Epoch 80/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6033 - accuracy: 0.8450\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5777 - accuracy: 0.8480\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.8490\n",
      "Epoch 83/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5694 - accuracy: 0.8450\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.8460\n",
      "Epoch 85/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.8470\n",
      "Epoch 86/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.8520\n",
      "Epoch 87/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5608 - accuracy: 0.8530\n",
      "Epoch 88/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5668 - accuracy: 0.8490\n",
      "Epoch 89/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5580 - accuracy: 0.8520\n",
      "Epoch 90/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.8510\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.8530\n",
      "Epoch 92/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5572 - accuracy: 0.8470\n",
      "Epoch 93/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5617 - accuracy: 0.8470\n",
      "Epoch 94/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.8480\n",
      "Epoch 95/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5597 - accuracy: 0.8550\n",
      "Epoch 96/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5514 - accuracy: 0.8520\n",
      "Epoch 97/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5583 - accuracy: 0.8480\n",
      "Epoch 98/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.8490\n",
      "Epoch 99/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5633 - accuracy: 0.8480\n",
      "Epoch 100/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.8490\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.8540\n",
      "Epoch 102/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.8550\n",
      "Epoch 103/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.8530\n",
      "Epoch 104/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5496 - accuracy: 0.8520\n",
      "Epoch 105/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.8520\n",
      "Epoch 106/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.8520\n",
      "Epoch 107/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.8490\n",
      "Epoch 108/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.8530\n",
      "Epoch 109/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5542 - accuracy: 0.8540\n",
      "Epoch 110/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5495 - accuracy: 0.8470\n",
      "Epoch 111/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5473 - accuracy: 0.8540\n",
      "Epoch 112/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5549 - accuracy: 0.8540\n",
      "Epoch 113/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.8490\n",
      "Epoch 114/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9837 - accuracy: 0.7650\n",
      "Epoch 115/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.3923 - accuracy: 0.6510\n",
      "Epoch 116/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8021 - accuracy: 0.7820\n",
      "Epoch 117/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6411 - accuracy: 0.8330\n",
      "Epoch 118/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5977 - accuracy: 0.8440\n",
      "Epoch 119/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5938 - accuracy: 0.8450\n",
      "Epoch 120/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.8480\n",
      "Epoch 121/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5605 - accuracy: 0.8460\n",
      "Epoch 122/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.8550\n",
      "Epoch 123/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5453 - accuracy: 0.8560\n",
      "Epoch 124/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5434 - accuracy: 0.8530\n",
      "Epoch 125/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5437 - accuracy: 0.8530\n",
      "Epoch 126/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5449 - accuracy: 0.8530\n",
      "Epoch 127/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.8580\n",
      "Epoch 128/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.8550\n",
      "Epoch 129/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.8550\n",
      "Epoch 130/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5391 - accuracy: 0.8520\n",
      "Epoch 131/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5403 - accuracy: 0.8600\n",
      "Epoch 132/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.8570\n",
      "Epoch 133/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5416 - accuracy: 0.8530\n",
      "Epoch 134/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.8580\n",
      "Epoch 135/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.8550\n",
      "Epoch 136/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5369 - accuracy: 0.8590\n",
      "Epoch 137/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5388 - accuracy: 0.8490\n",
      "Epoch 138/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.8580\n",
      "Epoch 139/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5369 - accuracy: 0.8530\n",
      "Epoch 140/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.8550\n",
      "Epoch 141/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.8540\n",
      "Epoch 142/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5488 - accuracy: 0.8490\n",
      "Epoch 143/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5401 - accuracy: 0.8560\n",
      "Epoch 144/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5372 - accuracy: 0.8550\n",
      "Epoch 145/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.8580\n",
      "Epoch 146/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.8550\n",
      "Epoch 147/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.8530\n",
      "Epoch 148/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5389 - accuracy: 0.8540\n",
      "Epoch 149/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5493 - accuracy: 0.8540\n",
      "Epoch 150/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.8440\n",
      "Epoch 151/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.0508 - accuracy: 0.7490\n",
      "Epoch 152/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.1037 - accuracy: 0.7190\n",
      "Epoch 153/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7090 - accuracy: 0.8080\n",
      "Epoch 154/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.8290\n",
      "Epoch 155/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.8420\n",
      "Epoch 156/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5461 - accuracy: 0.8530\n",
      "Epoch 157/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.8570\n",
      "Epoch 158/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.8540\n",
      "Epoch 159/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5366 - accuracy: 0.8590\n",
      "Epoch 160/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.8590\n",
      "Epoch 161/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.8600\n",
      "Epoch 162/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.8620\n",
      "Epoch 163/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.8550\n",
      "Epoch 164/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.8600\n",
      "Epoch 165/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5312 - accuracy: 0.8580\n",
      "Epoch 166/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.8590\n",
      "Epoch 167/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.8560\n",
      "Epoch 168/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.8590\n",
      "Epoch 169/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.8560\n",
      "Epoch 170/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5388 - accuracy: 0.8560\n",
      "Epoch 171/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5363 - accuracy: 0.8540\n",
      "Epoch 172/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.8600\n",
      "Epoch 173/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5346 - accuracy: 0.8580\n",
      "Epoch 174/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5305 - accuracy: 0.8620\n",
      "Epoch 175/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5293 - accuracy: 0.8600\n",
      "Epoch 176/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5294 - accuracy: 0.8600\n",
      "Epoch 177/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5293 - accuracy: 0.8570\n",
      "Epoch 178/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5295 - accuracy: 0.8610\n",
      "Epoch 179/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.8570\n",
      "Epoch 180/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.8530\n",
      "Epoch 181/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.8560\n",
      "Epoch 182/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5301 - accuracy: 0.8600\n",
      "Epoch 183/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5318 - accuracy: 0.8590\n",
      "Epoch 184/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5356 - accuracy: 0.8570\n",
      "Epoch 185/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.8590\n",
      "Epoch 186/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.8590\n",
      "Epoch 187/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5382 - accuracy: 0.8510\n",
      "Epoch 188/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.8500\n",
      "Epoch 189/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.5473 - accuracy: 0.8510\n",
      "Epoch 190/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5386 - accuracy: 0.8550\n",
      "Epoch 191/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5309 - accuracy: 0.8600\n",
      "Epoch 192/200\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5302 - accuracy: 0.8610\n",
      "Epoch 193/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5298 - accuracy: 0.8560\n",
      "Epoch 194/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.8570\n",
      "Epoch 195/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.8590\n",
      "Epoch 196/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.8580\n",
      "Epoch 197/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.8610\n",
      "Epoch 198/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.8590\n",
      "Epoch 199/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.8580\n",
      "Epoch 200/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.8590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15534d8e0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_X, train_Y_encoded, epochs=200, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21_input (InputLayer) [(None, 8, 8)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 6, 256)            6400      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 3, 128)            98432     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 117,732\n",
      "Trainable params: 117,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "predictions = Dense(100, activation='softmax')(cnn_model.layers[-3].output)\n",
    "model = Model(inputs=cnn_model.inputs, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrollment_data = enrollment_df.to_numpy()\n",
    "verification_data = verification_df.to_numpy()\n",
    "enrollment_data = enrollment_data[:, :-1]\n",
    "verification_data = verification_data[:, :-1]\n",
    "enrollment_data = np.reshape(enrollment_data, (185, 8, 8))\n",
    "verification_data = np.reshape(verification_data, (185, 8, 8))\n",
    "enrollment_pred = model.predict(enrollment_data)\n",
    "verification_pred = model.predict(verification_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 100)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_enrollment_pred = np.mean(enrollment_pred.reshape(-1, 5, 100), axis=1)\n",
    "agg_verification_pred = np.mean(verification_pred.reshape(-1, 5, 100), axis=1)\n",
    "agg_verification_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate (s, s) pairs\n",
    "authorized_pairs = []\n",
    "for i in range(len(agg_verification_pred)):\n",
    "    pair = (agg_enrollment_pred[i], agg_verification_pred[i])\n",
    "    authorized_pairs.append(pair)\n",
    "imposter_pairs = []\n",
    "# Generate (s, i) pairs\n",
    "for i in range(len(agg_verification_pred)):\n",
    "    random_i = i\n",
    "    while random_i == i:\n",
    "        random_i = np.random.randint(36)\n",
    "    pair = (agg_enrollment_pred[i], agg_verification_pred[random_i])\n",
    "    imposter_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Similarity score between 0 and 1 - 1 indicates identical, 0 indicates entirely different'''\n",
    "def dist_to_similarity(dist: float):\n",
    "    similarity = math.exp(-(dist**2))\n",
    "    return similarity\n",
    "\n",
    "def calc_false_rejection(threshold: float):\n",
    "    count = 0\n",
    "    for i in range(len(authorized_pairs)):\n",
    "        dist = np.linalg.norm(authorized_pairs[i][0] - authorized_pairs[i][1])\n",
    "        similarity = dist_to_similarity(dist)\n",
    "        if similarity < threshold:\n",
    "            # Authorized user is rejected\n",
    "            count += 1\n",
    "    return count/len(authorized_pairs)\n",
    "\n",
    "def calc_false_acceptance(threshold: float):\n",
    "    count = 0\n",
    "    for i in range(len(imposter_pairs)):\n",
    "        dist = np.linalg.norm(authorized_pairs[i][0] - authorized_pairs[i][1])\n",
    "        similarity = dist_to_similarity(dist)\n",
    "        if similarity > threshold:\n",
    "            count += 1\n",
    "    return count/len(imposter_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_rejection_scores = []\n",
    "false_acceptance_scores = []\n",
    "thresholds = [x * 0.05 for x in range(0, 40)]\n",
    "for threshold in thresholds:\n",
    "    false_rejection = calc_false_rejection(threshold)\n",
    "    false_accpetance = calc_false_acceptance(threshold)\n",
    "    false_rejection_scores.append(false_rejection)\n",
    "    false_acceptance_scores.append(false_accpetance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thresholds, false_rejection_scores, label='False Rejection')\n",
    "plt.plot(thresholds, false_acceptance_scores, label='False Acceptance')\n",
    "plt.title('FRR/FAR Curve')\n",
    "plt.legend(loc='center left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=64, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
